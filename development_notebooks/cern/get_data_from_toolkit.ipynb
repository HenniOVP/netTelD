{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "import unicodedata\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import threading\n",
    "import thread\n",
    "import itertools\n",
    "import math\n",
    "from bisect import bisect_left\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to get raw data from the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIndexClosestToTimestamp_mod(timestamp, dataPoints, collumThatIsNotNAN):\n",
    "    '''\n",
    "    Returns a data point from dataPoints, that is closest to the given timestamp.\n",
    "    As well the dataPoint will not be nan in the collum \"collumThatIsNotNAN\"\n",
    "    Complexity of this command: O( log( len(dataPoints) ) )\n",
    "    '''\n",
    "    timestamps_toSearch = dataPoints.axes[0]\n",
    "    cols = list(dataPoints.axes[1])\n",
    "    data = dataPoints.values\n",
    "    # bisect only works because we know our list is sorted\n",
    "    pos = bisect_left(timestamps_toSearch, timestamp)\n",
    "    if pos == len(dataPoints):\n",
    "        pos = len(dataPoints)-1\n",
    "    #print(pos)\n",
    "    # find the closest datapoint, that is not a NAN and return it\n",
    "    currentDataPoint = dict(zip(cols, dataPoints.values[pos]))\n",
    "    if not math.isnan(currentDataPoint[collumThatIsNotNAN]):\n",
    "        return pos\n",
    "    else:\n",
    "        pos = findIndexClostestThatIsNotNAN(pos, dataPoints, cols.index(collumThatIsNotNAN))\n",
    "        return pos\n",
    "\n",
    "def readPacketloss(file_lock, base_data=pd.DataFrame(), appendix=\"\"):\n",
    "    data = base_data.copy(deep=True)\n",
    "    with open(file_lock) as data_file:\n",
    "        data_raw = json.load(data_file)\n",
    "    for dataPoint in data_raw:\n",
    "        # skip summaries\n",
    "        if dataPoint['summary_window'] != 0:\n",
    "            continue\n",
    "        timestomp = datetime.strptime(dataPoint['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp_epoch = int(calendar.timegm(timestomp.utctimetuple()))\n",
    "        data.set_value(timestamp_epoch, \"packet_loss\"+appendix, dataPoint['value'])\n",
    "    return data\n",
    "\n",
    "def readTimeErrorEstimates(file_lock, base_data=pd.DataFrame(), appendix=\"\"):\n",
    "    data = base_data.copy(deep=True)\n",
    "    with open(file_lock) as data_file:\n",
    "        data_raw = json.load(data_file)\n",
    "    for dataPoint in data_raw:\n",
    "        # skip summaries\n",
    "        if dataPoint['summary_window'] != 0:\n",
    "            continue\n",
    "        timestomp = datetime.strptime(dataPoint['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp_epoch = int(calendar.timegm(timestomp.utctimetuple()))\n",
    "        data.set_value(timestamp_epoch, \"time_error_estimates\"+appendix, dataPoint['value'])\n",
    "    return data\n",
    "\n",
    "def readTTL(file_lock, base_data=pd.DataFrame(), appendix=\"\"):\n",
    "    data = base_data.copy(deep=True)\n",
    "    with open(file_lock) as data_file:\n",
    "        data_raw = json.load(data_file)\n",
    "    for dataPoint in data_raw:\n",
    "        # skip summaries\n",
    "        if dataPoint['summary_window'] != 0:\n",
    "            continue\n",
    "        timestomp = datetime.strptime(dataPoint['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp_epoch = int(calendar.timegm(timestomp.utctimetuple()))\n",
    "        avgStdMedian = getAvgStdMedianFromHistogram(dataPoint['value'])\n",
    "        data.set_value(timestamp_epoch, \"ttl_avg\"+appendix, avgStdMedian[0])\n",
    "        data.set_value(timestamp_epoch, \"ttl_std\"+appendix, avgStdMedian[1])\n",
    "        data.set_value(timestamp_epoch, \"ttl_median\"+appendix, avgStdMedian[2])\n",
    "    return data\n",
    "\n",
    "def readThroughput(file_lock, base_data=pd.DataFrame(), appendix=\"\"):\n",
    "    data = base_data.copy(deep=True)\n",
    "    with open(file_lock) as data_file:\n",
    "        data_raw = json.load(data_file)\n",
    "    for dataPoint in data_raw:\n",
    "        # skip summaries\n",
    "        if dataPoint['summary_window'] != 0:\n",
    "            continue\n",
    "        timestomp = datetime.strptime(dataPoint['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp_epoch = int(calendar.timegm(timestomp.utctimetuple()))\n",
    "        # put it to the closest appearence of another point, since we don't want nans...\n",
    "        pos = findIndexClosestToTimestamp_mod(timestamp_epoch, data, 'delay_avg')\n",
    "        target_timestamp = data.iloc[pos].name\n",
    "        data.set_value(target_timestamp, \"throughput_perfSonar\"+appendix, dataPoint['value'])\n",
    "    return data\n",
    "\n",
    "def readOWD(file_lock, base_data=pd.DataFrame(), appendix=\"\"):\n",
    "    data = base_data.copy(deep=True)\n",
    "    with open(file_lock) as data_file:\n",
    "        data_raw = json.load(data_file)\n",
    "    for dataPoint in data_raw:\n",
    "        # skip summaries\n",
    "        if dataPoint['summary_window'] != 0:\n",
    "            continue\n",
    "        timestomp = datetime.strptime(dataPoint['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        timestamp_epoch = int(calendar.timegm(timestomp.utctimetuple()))\n",
    "        avgStdMedian = getAvgStdMedianFromHistogram(dataPoint['value'])\n",
    "        data.set_value(timestamp_epoch, \"delay_avg\"+appendix, avgStdMedian[0])\n",
    "        data.set_value(timestamp_epoch, \"delay_std\"+appendix, avgStdMedian[1])\n",
    "        data.set_value(timestamp_epoch, \"delay_median\"+appendix, avgStdMedian[2])\n",
    "    return data\n",
    "\n",
    "def getAvgStdMedianFromHistogram(hitogram_dict):\n",
    "    histogram = np.array(hitogram_dict.items(), float)\n",
    "    values, weights_out = np.split(histogram, 2, axis=1)\n",
    "    avg = np.average(values, weights=weights_out)\n",
    "    std = np.sqrt(np.average((values-avg)**2, weights=weights_out))\n",
    "    summ = np.cumsum(weights_out)\n",
    "    index = np.searchsorted(summ, summ[len(summ)-1]/2)\n",
    "    median = values[index][0]\n",
    "    return avg, std, median\n",
    "\n",
    "\n",
    "def getRawDataFromArchive(src, dest):\n",
    "    data_in = pd.DataFrame()\n",
    "    data_out = pd.DataFrame()\n",
    "    for i in range(2):\n",
    "        data = pd.DataFrame()\n",
    "        start_date = \"September 11\"\n",
    "        command_latency = \"/home/hendrik/Downloads/esmond/esmond_client/clients/esmond-ps-get-bulk -u \"+src['latency_url']+\" -s \"+src['latency_IP']+\" -d \"+dest['latency_IP']+\" --start-time '\"+start_date+\"' --output-format=json -D /tmp\"\n",
    "        command_throughput = \"/home/hendrik/Downloads/esmond/esmond_client/clients/esmond-ps-get-bulk -u \"+src['throughput_url']+\" -s \"+src['throughput_IP']+\" -d \"+dest['throughput_IP']+\" --start-time '\"+start_date+\"' --output-format=json -D /tmp\"\n",
    "        \n",
    "        print(\"Getting raw data from archive: \" + src['name'])\n",
    "        call(command_latency, shell=True)\n",
    "        call(command_throughput, shell=True)\n",
    "\n",
    "        date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "        packetloss_file = \"/tmp/\"+src['latency_url'][7:]+\"_\"+dest['latency_url'][7:]+\"_packet-loss-rate_2016-09-11_\"+date_string+\".json\"\n",
    "        ttl_file = \"/tmp/\"+src['latency_url'][7:]+\"_\"+dest['latency_url'][7:]+\"_histogram-ttl_2016-09-11_\"+date_string+\".json\"\n",
    "        tee_file = \"/tmp/\"+src['latency_url'][7:]+\"_\"+dest['latency_url'][7:]+\"_time-error-estimates_2016-09-11_\"+date_string+\".json\"\n",
    "        owd_file = \"/tmp/\"+src['latency_url'][7:]+\"_\"+dest['latency_url'][7:]+\"_histogram-owdelay_2016-09-11_\"+date_string+\".json\"\n",
    "        throughput_file = \"/tmp/\"+src['throughput_url'][7:]+\"_\"+dest['throughput_url'][7:]+\"_throughput_2016-09-11_\"+date_string+\".json\"\n",
    "        \n",
    "\n",
    "        # read json file\n",
    "        print(\"Reading raw data files\")\n",
    "        data = readPacketloss(packetloss_file, base_data=data)\n",
    "        data = readTTL(ttl_file, base_data=data)\n",
    "        data = readTimeErrorEstimates(tee_file, base_data=data)\n",
    "        data = readOWD(owd_file, base_data=data)\n",
    "        data = readThroughput(throughput_file, base_data=data)\n",
    "        src, dest = dest, src\n",
    "        if i == 0:\n",
    "            data_out = data.copy(deep=True)\n",
    "        else:\n",
    "            data_in = data.copy(deep=True)\n",
    "    print(\"Sorting raw data\")\n",
    "    data_out.sort_index(inplace=True, ascending=False)\n",
    "    data_in.sort_index(inplace=True, ascending=False)\n",
    "    data = {'out': data_out, 'in': data_in}\n",
    "    with lock:\n",
    "        if src['name'] not in data_global.keys():\n",
    "            data_global[src['name']] = {}\n",
    "        data_global[src['name']][dest['name']] = data\n",
    "        print(\"Finished with: \"+ src['name']+ \" <-> \"+ dest['name'])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def starlikeConnector_linear(center_site_data, dest_data_list):\n",
    "    for dest_data in dest_data_list:\n",
    "        getRawDataFromArchive(center_site_data, dest_data)\n",
    "    \n",
    "def starlikeConnector_threaded(site_data, dest_data_list):\n",
    "    for dest_data in dest_data_list:\n",
    "        thread.start_new_thread(getRawDataFromArchive, (site_data, dest_data))\n",
    "        # rather use the new threading module for multithreading!\n",
    "        \n",
    "def allConnections_threaded(sites_list):\n",
    "    for sites in itertools.combinations(site_list, 2):\n",
    "        thread.start_new_thread(getRawDataFromArchive, (sites[0], sites[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting raw data from archive: CERNGetting raw data from archive: CERN\n",
      "Getting raw data from archive: CERNGetting raw data from archive: CERNGetting raw data from archive: CERNGetting raw data from archive: CERNGetting raw data from archive: CERN\n",
      "Getting raw data from archive: CERN\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Getting raw data from archive: JINR-T1\n",
      "Reading raw data files\n",
      "Getting raw data from archive: RAL\n",
      "Reading raw data files\n",
      "Getting raw data from archive: CCIN2P3\n",
      "Getting raw data from archive: TRIUMF\n",
      "Getting raw data from archive: BNL\n",
      "Getting raw data from archive: PIC\n",
      "Getting raw data from archive: CNAF\n",
      "Getting raw data from archive: KIT\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> JINR-T1\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> RAL\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Reading raw data files\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> CCIN2P3\n",
      "Reading raw data files\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> CNAF\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> KIT\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> PIC\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> BNL\n",
      "Sorting raw data\n",
      "Finished with: CERN <-> TRIUMF\n"
     ]
    }
   ],
   "source": [
    "src = {'name': 'CERN', 'latency_url': \"http://perfsonar-lt.cern.ch\", 'throughput_url': 'http://perfsonar-bw.cern.ch',\n",
    "      'latency_IP': '128.142.223.247', 'throughput_IP': '128.142.223.246'}\n",
    "dest1 = {'name': 'PIC', 'latency_url': \"http://psl01.pic.es\", 'throughput_url': 'http://psb01.pic.es',\n",
    "      'latency_IP': '193.109.172.188', 'throughput_IP': '193.109.172.187'}\n",
    "dest2 = {'name': 'RAL', 'latency_url': \"http://lcgps01.gridpp.rl.ac.uk\", 'throughput_url': 'http://lcgps02.gridpp.rl.ac.uk',\n",
    "      'latency_IP': '130.246.176.109', 'throughput_IP': '130.246.176.110'}\n",
    "dest3 = {'name': 'TRIUMF', 'latency_url': \"http://ps-latency.lhcmon.triumf.ca\", 'throughput_url': 'http://ps-bandwidth.lhcmon.triumf.ca',\n",
    "      'latency_IP': '206.12.9.2', 'throughput_IP': '206.12.9.1'}\n",
    "dest4 = {'name': 'KIT', 'latency_url': \"http://perfsonar2-de-kit.gridka.de\", 'throughput_url': 'http://perfsonar-de-kit.gridka.de',\n",
    "      'latency_IP': '192.108.47.12', 'throughput_IP': '192.108.47.6'}\n",
    "dest5 = {'name': 'CCIN2P3', 'latency_url': \"http://ccperfsonar2.in2p3.fr\", 'throughput_url': 'http://ccperfsonar1.in2p3.fr',\n",
    "      'latency_IP': '193.48.99.76', 'throughput_IP': '193.48.99.77'}\n",
    "dest6 = {'name': 'BNL', 'latency_url': \"http://lhcperfmon.bnl.gov\", 'throughput_url': 'http://lhcmon.bnl.gov',\n",
    "      'latency_IP': '192.12.15.26', 'throughput_IP': '192.12.15.23'}\n",
    "dest7 = {'name': 'CNAF', 'latency_url': \"http://perfsonar-ow.cnaf.infn.it\", 'throughput_url': 'http://perfsonar-ps.cnaf.infn.it',\n",
    "      'latency_IP': '131.154.254.12', 'throughput_IP': '131.154.254.11'}\n",
    "dest8 = {'name': 'JINR-T1', 'latency_url': \"http://t1-pfsn2.jinr-t1.ru\", 'throughput_url': 'http://t1-pfsn1.jinr-t1.ru',\n",
    "      'latency_IP': '159.93.229.151', 'throughput_IP': '159.93.229.150'}\n",
    "dests = [dest1, dest2, dest3, dest4, dest5, dest6, dest7, dest8]\n",
    "\n",
    "\n",
    "lock = threading.Lock()\n",
    "data_global = {}\n",
    "starlikeConnector_threaded(src, dests)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(data_global, open('raw_toolkit_CERN_to_RAL-PIC-TRIUMF-KIT-IN2P3-BNL-CNAF-JINR-T1_sorted.pkl', \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN <-> TRIUMF | RAL | PIC | JINR-T1 | KIT | BNL | CCIN2P3 | CNAF |  \n"
     ]
    }
   ],
   "source": [
    "# test which keys we have\n",
    "for key1 in data_global.keys():\n",
    "    print(key1 + \" <-> \", end=\"\")\n",
    "    for key2 in data_global[key1].keys():\n",
    "        print(key2 + \" | \", end=\"\")\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
